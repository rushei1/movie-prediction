{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M3qGCp4z2vHF"
   },
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "cIBVXTSw2vHM",
    "outputId": "938892ea-4001-4fa8-8ffc-baae82dc9417"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ead32f3c1253>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Splitting the dataset into the Training set and Test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# test_size = 0.2 means 20% data is in test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Importing the data set\n",
    "dataset = pd.read_csv(\"datasetfinal.csv\")\n",
    "\n",
    "# Split dataset into X (independent variable ) and y (dependent variable)\n",
    "X = dataset.iloc[:, :-1].values #loc/iloc\n",
    "y = dataset.iloc[:, 11].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "number = LabelEncoder()\n",
    "nameencoder=LabelEncoder()\n",
    "actor1encoder=LabelEncoder()\n",
    "actor2encoder=LabelEncoder()\n",
    "actor3encoder=LabelEncoder()\n",
    "genresencoder=LabelEncoder()\n",
    "imdbscoreencoder=LabelEncoder()\n",
    "budgetencoder=LabelEncoder()\n",
    "grossencoder=LabelEncoder()\n",
    "profitencoder =LabelEncoder()\n",
    "\n",
    "#Encoding each categorical features. \n",
    "dataset['director_name'] = nameencoder.fit_transform(dataset['director_name'])\n",
    "dataset['actor_1_name'] = actor1encoder.fit_transform(dataset['actor_1_name'])\n",
    "dataset['actor_2_name'] = actor2encoder.fit_transform(dataset['actor_2_name'].astype(str))\n",
    "dataset['actor_3_name'] = actor3encoder.fit_transform(dataset['actor_3_name'].astype(str))\n",
    "dataset['genres'] = genresencoder.fit_transform(dataset['genres'])\n",
    "\n",
    "# dataset['imdb_score'] = imdbscoreencoder.fit_transform(dataset['imdb_score'])\n",
    "# dataset['budget'] = budgetencoder.fit_transform(dataset['budget'])\n",
    "# dataset['gross'] = grossencoder.fit_transform(dataset['gross'])\n",
    "# dataset['profit_percent'] = profitencoder.fit_transform(dataset['profit_percent'])\n",
    "# Deal with all features. You skipped actor_2_name, actor_3_name. Take care of missing datas in actor_2 and actor_3.\n",
    "features = [\"director_name\", \"actor_1_name\", \"genres\",\"imdb_score\",\"budget\",\"gross\",\"profit_percent\"]\n",
    "\n",
    "# Encoding the Dependent Variable\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[features], y, test_size = 0.2, random_state = 0) \n",
    "# test_size = 0.2 means 20% data is in test set\n",
    "# random_state = 0 means it will generate same test set and train set\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Scalind the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict values using the training data\n",
    "nb_predict_train = model.predict(X_test)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "#Predict Output \n",
    "# During training we have given features [\"director_name\", \"actor_1_name\", \"genres\",\"imdb_score\",\"budget\",\"gross\",\"profit_percent\"]\n",
    "# So to predict provide same fearures in same order. \n",
    "\n",
    "\n",
    "actor_name= input(\"Director Name        : \")\n",
    "director_name= input(\"Actor Name           : \")\n",
    "genre= input(\"Genre                : \")\n",
    "imdb_rating= float(input(\"IMDB Rating          : \"))\n",
    "budget= float(input(\"Budget               : \"))\n",
    "gross= float(input(\"Gross                : \"))\n",
    "profit_percent= float(input(\"Profit Percentage   : \"))\n",
    "# predict=[\"James Cameron\",\"CCH Pounder\",\"Action|Adventure|Fantasy|Sci-Fi\",7.9,237000000, 760505847, 0.031347]\n",
    "# predict=[\"Sam Mendes\",\"Christoph Waltz\",\"Action|Adventure|Thriller\",6.8,245000000,200074175,-0.183370714]\n",
    "# predict=[\"Andrew Stanton\",\"Daryl Sabara\", \"Action|Adventure|Sci-Fi\",6.6,263700000,73058679,-0.722947747]\n",
    "# predict=[\"James Cameron\",\"Daryl Sabara\",\"Action|Adventure|Thriller\",5.5,26589565,584565,0.032]\n",
    "\n",
    "predict=[actor_name,director_name,genre,imdb_rating,budget,gross,profit_percent]\n",
    "\n",
    "predict[0]=nameencoder.transform([predict[0]])\n",
    "predict[1]=actor1encoder.transform([predict[1]])\n",
    "predict[2]= genresencoder.transform([predict[2]])\n",
    "# Since the below ones are numerals no need to label encode them\n",
    "# predict[3]=imdbscoreencoder.transform([predict[3]])\n",
    "# predict[4]=budgetencoder.transform([predict[4]])\n",
    "# predict[5]=grossencoder.transform([predict[5]])\n",
    "# predict[6]=profitencoder.transform([0.031347])\n",
    "\n",
    "# Scale or normalize the datas. \n",
    "predict = scaler.transform([predict])\n",
    "prediction = model.predict(predict)\n",
    "\n",
    "\n",
    "if prediction == 1:\n",
    "    print(\"                           HIT\")\n",
    "else:\n",
    "    print(\"                           FLOP\")\n",
    "    \n",
    "          \n",
    "# Accuracy\n",
    "print(\"                             ACCURACY: {0:.4f}\".format(metrics.accuracy_score(y_test, nb_predict_train)))\n",
    "print()          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5kkwt7W3M9j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
